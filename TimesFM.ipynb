{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUCiNbH0c/xpM6P/p+l1Q4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ziraax/AcademicSuccessPrediction/blob/main/TimesFM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook attend to replicate the model presented in the paper : [A DECODER-ONLY FOUNDATION MODEL FOR TIME-SERIES FORECASTING](https://arxiv.org/pdf/2310.10688) from **Google Research** using **Pytorch**.\n",
        "Note that everything here is granted as-is, and without any guarantee.\n",
        "\n"
      ],
      "metadata": {
        "id": "KIsQJuUNfz5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summary of the Paper**\n",
        "\n",
        "The paper, titled \"A Decoder-Only Foundation Model for Time-Series Forecasting,\" presents TimesFM, a foundation model designed for time-series forecasting. This model leverages recent advancements in large language models (LLMs) for NLP to build a versatile time-series forecasting model that performs well across various datasets and scenarios without the need for fine-tuning or dataset-specific training.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "### Motivation and Objective:\n",
        "  - Goal: Develop a zero-shot time-series forecasting model that performs competitively with state-of-the-art supervised models on unseen datasets.\n",
        "  - Challenges: Unlike NLP, time-series data lacks a standardized vocabulary or grammar, and there is limited availability of large-scale public time-series datasets.\n",
        "\n",
        "### Model Design:\n",
        "  - Architecture: Decoder-only transformer model inspired by language models but adapted for time-series data.\n",
        "  - Patching: Time-series data is broken into patches, akin to tokens in NLP models, to manage long sequences and improve model efficiency.\n",
        "  - Decoder-Only Approach: The model uses a decoder-only architecture for efficient training and prediction.\n",
        "\n",
        "### Training Data:\n",
        "  - Dataset Composition: Combines real-world (e.g., web search queries, Wikipedia page visits) and synthetic time-series data to create a large and diverse training corpus.\n",
        "\n",
        "### Model Characteristics:\n",
        "  - Flexibility: Capable of handling varying context lengths, prediction horizons, and time granularities.\n",
        "  - Efficiency: Despite being smaller in parameter size and data volume compared to typical LLMs, TimesFM achieves competitive zero-shot performance.\n",
        "\n",
        "### Comparison with Existing Models:\n",
        "  - Performance: Demonstrates superior zero-shot performance compared to LLM-based forecasters (e.g., GPT-3, LLaMA-2) at a fraction of the cost.\n",
        "  - State-of-the-Art: Matches or nearly matches the accuracy of best-in-class supervised models on various forecasting tasks.\n",
        "\n",
        "# Detailed Explanation\n",
        "  \n",
        "### Introduction and Background:\n",
        "  - Highlights the ubiquity and importance of time-series data in domains like retail, finance, healthcare, and more.\n",
        "  - Discusses the rise of deep learning models in time-series forecasting and their advantages over classical statistical methods like ARIMA and GARCH.\n",
        "  - Draws parallels with the success of large language models in NLP to motivate the development of a time-series foundation model.\n",
        "\n",
        "### Related Work:\n",
        "  - Reviews previous approaches in time-series forecasting, including local univariate models, global univariate models, and global multivariate models.\n",
        "  - Notes recent attempts to use pretrained LLMs for time-series forecasting, emphasizing the novelty and efficiency of TimesFM in this context.\n",
        "\n",
        "### Model Architecture:\n",
        "  - Input Layer: Processes time-series into patches, each processed into a vector by a residual block.\n",
        "  - Stacked Transformer Layers: Utilizes multi-head self-attention with causal attention to ensure each output token attends to only preceding tokens.\n",
        "  - Output Layer: Maps the encoded information into future time-series predictions.\n",
        "  - Loss Function: Uses Mean Squared Error (MSE) for point forecasting, with flexibility for probabilistic forecasting if needed.\n",
        "\n",
        "### Training Strategy:\n",
        "  - Describes the use of mini-batch gradient descent and a unique patch masking strategy to ensure the model learns across varying context lengths.\n",
        "\n",
        "### Experiments and Results:\n",
        "  - Dataset Variety: Evaluates the model on diverse unseen datasets, demonstrating robust zero-shot performance.\n",
        "  - Performance Metrics: Compares TimesFM's accuracy to supervised models, showing close or superior results.\n",
        "\n",
        "### Conclusion:\n",
        "  - Summarizes the effectiveness of TimesFM as a practical, efficient solution for time-series forecasting.\n",
        "  - Suggests potential future directions, including further scaling and fine-tuning for specific applications.\n",
        "\n",
        "### Implications and Benefits\n",
        "  - Efficiency: TimesFM provides a highly efficient alternative to traditional supervised models, significantly reducing training and computational costs.\n",
        "  - Versatility: The model's ability to perform well across different datasets and scenarios without additional training makes it highly versatile for real-world applications.\n",
        "  - Foundation for Future Work: Sets a new benchmark for zero-shot time-series forecasting, paving the way for further research and development in this area.\n",
        "\n",
        "## More on the model architecture\n",
        "\n",
        "TimesFM leverages a decoder-only transformer architecture, which is inspired by large language models (LLMs) but adapted for the unique characteristics of time-series data. The key components of the architecture include the input processing, transformer layers, and the output generation.\n",
        "\n",
        "### Detailed Components\n",
        "\n",
        "  1. **Input Processing:**\n",
        "\n",
        "    *Time-Series Patching*:\n",
        "        The continuous time-series data is divided into fixed-size patches. Each patch represents a segment of the time-series data.\n",
        "        This approach is akin to tokenizing text in NLP models, where patches serve as the basic units of input.\n",
        "\n",
        "    *Embedding*:\n",
        "        Each patch is embedded into a higher-dimensional space using a linear embedding layer. This transforms the raw time-series values into vectors that the model can process.\n",
        "        The embedding layer is followed by a residual block that helps stabilize the training and enhances the model's ability to capture complex patterns.\n",
        "\n",
        "  2. **Transformer Layers:**\n",
        "\n",
        "    *Multi-Head Self-Attention*:\n",
        "        Self-attention allows the model to weigh the importance of different patches when making predictions. In the context of time-series, it helps the model understand dependencies across different time steps.\n",
        "        Multi-head attention involves multiple attention mechanisms running in parallel, providing the model with the ability to capture various aspects of the data's structure.\n",
        "\n",
        "    *Causal Attention*:\n",
        "        Causal attention ensures that each output only depends on the current and past inputs, preventing information leakage from future patches. This is crucial for time-series forecasting, where predictions at a given time should not be influenced by future data.\n",
        "\n",
        "    *Feed-Forward Neural Network*:\n",
        "        Each transformer layer includes a position-wise feed-forward network, applied independently to each position (patch) in the sequence.\n",
        "        The feed-forward network consists of two linear transformations with a ReLU activation in between.\n",
        "\n",
        "    *Residual Connections and Layer Normalization*:\n",
        "        Residual connections are used around each sub-layer (multi-head attention and feed-forward network) to facilitate gradient flow and prevent vanishing gradients.\n",
        "        Layer normalization is applied to stabilize and speed up training.\n",
        "\n",
        "  3. **Output Generation:**\n",
        "\n",
        "    *Prediction Layer*:\n",
        "        The final layer of the model maps the output from the transformer layers to the predicted time-series values. This is typically a linear transformation.\n",
        "        For point forecasting, the model directly outputs the future values. For probabilistic forecasting, it can output parameters of a probability distribution.\n",
        "\n",
        "    *Loss Function*:\n",
        "        The primary loss function used is Mean Squared Error (MSE), which measures the average squared difference between the predicted and actual values. MSE is suitable for point forecasts where the objective is to minimize the prediction error.\n",
        "        The model is flexible enough to accommodate other loss functions for different forecasting objectives, such as probabilistic forecasts.\n",
        "\n",
        "  4. **Advantages of the Architecture**\n",
        "\n",
        "    *Scalability*:\n",
        "        The transformer architecture is highly scalable, allowing the model to handle long time-series by processing patches in parallel. This is essential for efficiently dealing with large datasets.\n",
        "\n",
        "    *Flexibility*:\n",
        "        By using a decoder-only approach, TimesFM can generate predictions iteratively, making it suitable for a variety of forecasting horizons and granularities.\n",
        "        The model's structure allows for handling diverse time-series data without needing dataset-specific customization.\n",
        "\n",
        "    *Efficiency*:\n",
        "        Despite being smaller in size compared to typical LLMs, TimesFM achieves competitive performance due to its efficient use of the transformer architecture and effective training strategies.\n",
        "\n",
        "    *Generalization*:\n",
        "        The use of diverse training data (both real-world and synthetic) enables the model to generalize well to unseen datasets, providing robust zero-shot performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "1ETidOXOgX_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ** Summary of the Paper\n",
        "\n",
        "The paper, titled \"A Decoder-Only Foundation Model for Time-Series Forecasting,\" presents TimesFM, a foundation model designed for time-series forecasting. This model leverages recent advancements in large language models (LLMs) for NLP to build a versatile time-series forecasting model that performs well across various datasets and scenarios without the need for fine-tuning or dataset-specific training.\n",
        "Key Components\n",
        "\n",
        "    Motivation and Objective:\n",
        "        Goal: Develop a zero-shot time-series forecasting model that performs competitively with state-of-the-art supervised models on unseen datasets.\n",
        "        Challenges: Unlike NLP, time-series data lacks a standardized vocabulary or grammar, and there is limited availability of large-scale public time-series datasets.\n",
        "\n",
        "    Model Design:\n",
        "        Architecture: Decoder-only transformer model inspired by language models but adapted for time-series data.\n",
        "        Patching: Time-series data is broken into patches, akin to tokens in NLP models, to manage long sequences and improve model efficiency.\n",
        "        Decoder-Only Approach: The model uses a decoder-only architecture for efficient training and prediction.\n",
        "\n",
        "    Training Data:\n",
        "        Dataset Composition: Combines real-world (e.g., web search queries, Wikipedia page visits) and synthetic time-series data to create a large and diverse training corpus.\n",
        "\n",
        "    Model Characteristics:\n",
        "        Flexibility: Capable of handling varying context lengths, prediction horizons, and time granularities.\n",
        "        Efficiency: Despite being smaller in parameter size and data volume compared to typical LLMs, TimesFM achieves competitive zero-shot performance.\n",
        "\n",
        "    Comparison with Existing Models:\n",
        "        Performance: Demonstrates superior zero-shot performance compared to LLM-based forecasters (e.g., GPT-3, LLaMA-2) at a fraction of the cost.\n",
        "        State-of-the-Art: Matches or nearly matches the accuracy of best-in-class supervised models on various forecasting tasks.\n",
        "\n",
        "Detailed Explanation\n",
        "\n",
        "    Introduction and Background:\n",
        "        Highlights the ubiquity and importance of time-series data in domains like retail, finance, healthcare, and more.\n",
        "        Discusses the rise of deep learning models in time-series forecasting and their advantages over classical statistical methods like ARIMA and GARCH.\n",
        "        Draws parallels with the success of large language models in NLP to motivate the development of a time-series foundation model.\n",
        "\n",
        "    Related Work:\n",
        "        Reviews previous approaches in time-series forecasting, including local univariate models, global univariate models, and global multivariate models.\n",
        "        Notes recent attempts to use pretrained LLMs for time-series forecasting, emphasizing the novelty and efficiency of TimesFM in this context.\n",
        "\n",
        "    Model Architecture:\n",
        "        Input Layer: Processes time-series into patches, each processed into a vector by a residual block.\n",
        "        Stacked Transformer Layers: Utilizes multi-head self-attention with causal attention to ensure each output token attends to only preceding tokens.\n",
        "        Output Layer: Maps the encoded information into future time-series predictions.\n",
        "        Loss Function: Uses Mean Squared Error (MSE) for point forecasting, with flexibility for probabilistic forecasting if needed.\n",
        "\n",
        "    Training Strategy:\n",
        "        Describes the use of mini-batch gradient descent and a unique patch masking strategy to ensure the model learns across varying context lengths.\n",
        "\n",
        "    Experiments and Results:\n",
        "        Dataset Variety: Evaluates the model on diverse unseen datasets, demonstrating robust zero-shot performance.\n",
        "        Performance Metrics: Compares TimesFM's accuracy to supervised models, showing close or superior results.\n",
        "\n",
        "    Conclusion:\n",
        "        Summarizes the effectiveness of TimesFM as a practical, efficient solution for time-series forecasting.\n",
        "        Suggests potential future directions, including further scaling and fine-tuning for specific applications.\n",
        "\n",
        "Implications and Benefits\n",
        "\n",
        "    Efficiency: TimesFM provides a highly efficient alternative to traditional supervised models, significantly reducing training and computational costs.\n",
        "    Versatility: The model's ability to perform well across different datasets and scenarios without additional training makes it highly versatile for real-world applications.\n",
        "    Foundation for Future Work: Sets a new benchmark for zero-shot time-series forecasting, paving the way for further research and development in this area.\n",
        "\n",
        "By building on the principles of large language models and adapting them to the unique challenges of time-series data, TimesFM represents a significant advancement in the field of time-series forecasting.\n"
      ],
      "metadata": {
        "id": "OP03l83pgOOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKfInfcuftW4"
      },
      "outputs": [],
      "source": []
    }
  ]
}